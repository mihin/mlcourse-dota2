{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src='http://www.thumbnailtemplates.com/images/thumbs/thumb-099-dota-2-2.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dota 2 winner prediction\n",
    "    \n",
    "In this Kernel I build a simple multilayer neural network (multilayer perceptron) in Pytorch to predict which team (Radiant or Dire) will win a Dota 2 match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "import datetime\n",
    "import pytz\n",
    "import time\n",
    "import random\n",
    "\n",
    "# PyTorch stuff\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "# Sklearn stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "PATH_TO_DATA = '../input'\n",
    "print(os.listdir(PATH_TO_DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "df_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_features.csv'), \n",
    "                                    index_col='match_id_hash')\n",
    "df_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_targets.csv'), \n",
    "                                   index_col='match_id_hash')\n",
    "\n",
    "# Test dataset\n",
    "df_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), \n",
    "                                   index_col='match_id_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is missing data\n",
    "print(df_train_features.isnull().values.any())\n",
    "print(df_test_features.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine train and test datasets in one dataset. This allows for addding new features for both datasets at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_features = pd.concat([df_train_features, df_test_features])\n",
    "\n",
    "# Index to split the training and test data sets\n",
    "idx_split = df_train_features.shape[0]\n",
    "\n",
    "# That is, \n",
    "# df_train_features == df_full_features[:idx_split]\n",
    "# df_test_features == df_full_features[idx_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the game-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_features.drop(['game_time', 'game_mode', 'lobby_type', 'objectives_len', 'chat_len'],\n",
    "                      inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for writing to submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels):\n",
    "    df_submission = pd.DataFrame({'radiant_win_prob': predicted_labels}, \n",
    "                                     index=df_test_features.index)\n",
    "\n",
    "    submission_filename = 'submission_{}.csv'.format(\n",
    "        datetime.datetime.now(tz=pytz.timezone('Europe/Athens')).strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    \n",
    "    df_submission.to_csv(submission_filename)\n",
    "    \n",
    "    print('Submission saved to {}'.format(submission_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the player-features\n",
    "\n",
    "There are 245 different features in the dataset. It might be daunting at first, however there are 10 different players (2 teams with 5 players in each), and there are only 24 unique player-features for each player (which gives us 240 player-features in total). The remaining 5 features are general game-features which I don't use here.\n",
    "\n",
    "The description of each player-feature is given in the following table (source: dota2.gamepedia.com). This might be helpfull for people who have never played Dota 2 like me.\n",
    "\n",
    "\n",
    "|  Feature  | Description |\n",
    "| ------------- |:-------------| \n",
    "| **hero_id** | ID of player's hero (int64). [Heroes](https://dota2.gamepedia.com/Heroes) are the essential element of Dota 2, as the course of the match is dependent on their intervention. During a match, two opposing teams select five out of 117 heroes that accumulate experience and gold to grow stronger and gain new abilities in order to destroy the opponent's Ancient. Most heroes have a distinct role that defines how they affect the battlefield, though many heroes can perform multiple roles. A hero's appearance can be modified with equipment.|\n",
    "| **kills** | Number of killed players (int64).|\n",
    "| **deaths** | Number of deaths of the player (int64).|\n",
    "| **gold** | Amount of gold (int64). [Gold](https://dota2.gamepedia.com/Gold) is the currency used to buy items or instantly revive your hero. Gold can be earned from killing heroes, creeps, or buildings. |\n",
    "| **xp** | Experience points (int64). [Experience](https://dota2.gamepedia.com/Experience) is an element heroes can gather by killing enemy units, or being present as enemy units get killed. On its own, experience does nothing, but when accumulated, it increases the hero's level, so that they grow more powerful.   |\n",
    "| **lh** | Number of last hits (int64). [Last-hitting](https://dota2.gamepedia.com/Creep_control_techniques#Last-hitting) is a technique where you (or a creep under your control) get the 'last hit' on a neutral creep, enemy lane creep, or enemy hero. The hero that dealt the killing blow to the enemy unit will be awarded a bounty.|\n",
    "| **denies** | Number of denies (int64). [Denying](https://dota2.gamepedia.com/Denying) is the act of preventing enemy heroes from getting the last hit on a friendly unit by last hitting the unit oneself. Enemies earn reduced experience if the denied unit is not controlled by a player, and no experience if it is a player controlled unit. Enemies gain no gold from any denied unit. |\n",
    "| **assists** | Number of [assists](https://dota2.gamepedia.com/Gold#Assists_.28AoE_gold.29) (int64). Allied heroes within 1300 radius of a killed enemy, including the killer, receive experience and reliable gold if they assisted in the kill. To qualify for an assist, the allied hero merely has to be within the given radius of the dying enemy hero. |\n",
    "| **health** | Health points (int64). [Health](https://dota2.gamepedia.com/Health) represents the life force of a unit. When a unit's current health reaches 0, it dies. Every hero has a base health pool of 200. This value exists for all heroes and cannot be altered. This means that a hero's maximum health cannot drop below 200. |\n",
    "| **max_health** | Hero's maximum health pool (int64).|\n",
    "| **max_mana** | Hero's maximum mana pool (float64). [Mana](https://dota2.gamepedia.com/Mana) represents the magic power of a unit. It is used as a cost for the majority of active and even some passive abilities. Every hero has a base mana pool of 75, while most non-hero units only have a set mana pool if they have abilities which require mana, with a few exceptions. These values cannot be altered. This means that a hero's maximum mana cannot drop below 75. |\n",
    "| **level** | [Level](https://dota2.gamepedia.com/Experience#Leveling) of player's hero (int64). Each hero begins at level 1, with one free ability point to spend. Heroes may level up by acquiring certain amounts of experience. Upon leveling up, the hero's attributes increase by fixed amounts (unique for each hero), which makes them overall more powerful. Heroes may also gain more ability points by leveling up, allowing them to learn new spells, or to improve an already learned spell, making it more powerful. Heroes can gain a total for 24 levels, resulting in level 25 as the highest possible level a hero can reach. |\n",
    "| **x** | Player's X coordinate (int64) |\n",
    "| **y** | Player's Y coordinate (int64) |\n",
    "| **stuns** | Total stun duration? (float64). [Stun](https://dota2.gamepedia.com/Stun) is a status effect that completely locks down affected units, disabling almost all of its capabilities. |\n",
    "| **creeps_stacked** | Number of stacked creeps (int64). [Creep Stacking](https://dota2.gamepedia.com/Creep_Stacking) is the process of drawing neutral creeps away from their camps in order to increase the number of units in an area. By pulling neutral creeps beyond their camp boundaries, the game will generate a new set of creeps for the player to interact with in addition to any remaining creeps. This is incredibly time efficient, since it effectively increases the amount of gold available for a team. |\n",
    "| **camps_stacked** | Number of stacked camps  (int64). |\n",
    "| **rune_pickups** | Number of picked up [runes](https://dota2.gamepedia.com/Runes)  (int64).  |\n",
    "| **firstblood_claimed** | boolean feature? (int64) |\n",
    "| **teamfight_participation** |  Team fight participation rate? (float64) |\n",
    "| **towers_killed** | Number of killed/destroyed Towers (int64). [Towers](https://dota2.gamepedia.com/Buildings#Towers) are the main line of defense for both teams, attacking any non-neutral enemy that gets within their range. Both factions have all three lanes guarded by three towers each. Additionally, each faction's Ancient have two towers as well, resulting in a total of 11 towers per faction. Towers come in 4 different tiers. |\n",
    "| **roshans_killed** | Number of killed Roshans  (int64). [Roshan](https://dota2.gamepedia.com/Roshan) is the most powerful neutral creep in Dota 2. It is the first unit which spawns, right as the match is loaded. During the early to mid game, he easily outmatches almost every hero in one-on-one combat. Very few heroes can take him on alone during the mid-game. Even in the late game, lots of heroes struggle fighting him one on one, since Roshan grows stronger as time passes. |\n",
    "| **obs_placed** | Number of observer-wards placed by a player (int64). [Observer Ward](https://dota2.gamepedia.com/Observer_Ward), an invisible watcher that gives ground vision in a 1600 radius to your team. Lasts 6 minutes. |\n",
    "| **sen_placed** | Number of sentry-wards placed by a player (int64) [Sentry Ward](https://dota2.gamepedia.com/Sentry_Ward), an invisible watcher that grants True Sight, the ability to see invisible enemy units and wards, to any existing allied vision within a radius. Lasts 6 minutes.|\n",
    "\n",
    "**Note**: I am not sure about the meaning of some features: `stuns`, `firstblood_claimed` and `teamfight_participation`. Also, the number of towers killed by a team in a few cases is 12, whereas according to wiki the number of towers of each team is 11.\n",
    "Please correct me if I am wrong in the comments and help to clarrify the meaning of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess features\n",
    "Clearly the `hero_id` is a categorical feature, so let's one-hot encode it. Note that according to wiki there are 117 heroes, however in our dataset there are 116 heroes with ids `1, 2, ..., 114, 119, 120`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will get the same result for all teams and players, here I use r1.\n",
    "np.sort(np.unique(df_full_features['r1_hero_id'].values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['r', 'd']:\n",
    "    for i in range(1, 6):\n",
    "        df_full_features = pd.get_dummies(df_full_features, columns = [f'{t}{i}_hero_id'])\n",
    "#         df_full_features = pd.concat([df_full_features,\n",
    "#                                       pd.get_dummies(df_full_features[f'{t}{i}_hero_id'], prefix=f'{t}{i}_hero_id')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's scale the player-features that have relatively large values, such as `gold`, `lh`, `xp` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features = set(f[3:] for f in df_train_features.columns[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_scale = []\n",
    "for t in ['r', 'd']:\n",
    "    for i in range(1, 6):\n",
    "        for f in player_features - {'hero_id', 'firstblood_claimed', 'teamfight_participation'}:\n",
    "            features_to_scale.append(f'{t}{i}_{f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_features_scaled = df_full_features.copy()\n",
    "df_full_features_scaled[features_to_scale] = MinMaxScaler().fit_transform(df_full_features_scaled[features_to_scale])  # alternatively use StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_features_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_features_scaled.max().sort_values(ascending=False).head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct X and y arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_full_features_scaled[:idx_split]\n",
    "X_test = df_full_features_scaled[idx_split:]\n",
    "\n",
    "y_train = df_train_targets['radiant_win'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "<img src='https://www.vaetas.cz/img/machine-learning/multilayer-perceptron.png' >\n",
    "\n",
    "Let's build a feedforward neural network – a multilayer perceptron (MLP for short).  Pytorch provides a convinient and easy way to do it with the help of the [nn.Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential) class, which is a sequential container of different modules (nn.Module’s) – building blocks of a neural network. Below is a simple MLP with one input layer (4 nodes), two hidden layers (4 nodes each) with ReLU activation, and an output layer with Sigmoid activation (1 node, for a binary classification problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.Sequential(nn.Linear(6, 4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4, 4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4, 1),\n",
    "                    nn.Sigmoid()\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using `nn.Sequential`, let's define our own MLP class which will allow us to build a MLP just by passing any number of hidden layers and nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    ''' Multi-layer perceptron with ReLu and Softmax.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        n_input (int): number of nodes in the input layer \n",
    "        n_hidden (int list): list of number of nodes n_hidden[i] in the i-th hidden layer \n",
    "        n_output (int):  number of nodes in the output layer \n",
    "        drop_p (float): drop-out probability [0, 1]\n",
    "        random_state (int): seed for random number generator (use for reproducibility of result)\n",
    "    '''\n",
    "    def __init__(self, n_input, n_hidden, n_output, drop_p, random_state=SEED):\n",
    "        super().__init__()   \n",
    "        self.random_state = random_state\n",
    "        set_random_seed(SEED)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(n_input, n_hidden[0])])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in zip(n_hidden[:-1], n_hidden[1:])])\n",
    "        self.output_layer = nn.Linear(n_hidden[-1], n_output)       \n",
    "        self.dropout = nn.Dropout(p=drop_p)  # method to prevent overfitting\n",
    "                \n",
    "    def forward(self, X):\n",
    "        ''' Forward propagation -- computes output from input X.\n",
    "        '''\n",
    "        for h in self.hidden_layers:\n",
    "            X = F.relu(h(X))\n",
    "            X = self.dropout(X)\n",
    "        X = self.output_layer(X)\n",
    "        return torch.sigmoid(X)\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        return self.forward(X_test).detach().squeeze(1).numpy()\n",
    "    \n",
    "    \n",
    "\n",
    "def set_random_seed(rand_seed=SEED):\n",
    "    ''' Helper function for setting random seed. Use for reproducibility of results'''\n",
    "    if type(rand_seed) == int:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        random.seed(rand_seed)\n",
    "        np.random.seed(rand_seed)\n",
    "        torch.manual_seed(rand_seed)\n",
    "        torch.cuda.manual_seed(rand_seed)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function for training our MLP and a function for plotting training and validation losses with respect to the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, criterion, optimizer, scheduler, dataloaders, verbose=False):\n",
    "    ''' \n",
    "    Train the given model...\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        model: model (MLP) to train\n",
    "        epochs (int): number of epochs\n",
    "        criterion: loss function e.g. BCELoss\n",
    "        optimizer: optimizer e.g SGD or Adam \n",
    "        scheduler: learning rate scheduler e.g. StepLR\n",
    "        dataloaders: train and validation dataloaders\n",
    "        verbose (boolean): print training details (elapsed time and losses)\n",
    "\n",
    "    '''\n",
    "    t0_tot = time.time()\n",
    "    \n",
    "    set_random_seed(model.random_state)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Training on {device}...')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Best model weights (deepcopy them because model.state_dict() changes during the training)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
    "    best_loss = np.inf\n",
    "    losses = {'train': [], 'valid': []}\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        t0 = time.time()\n",
    "        print(f'============== Epoch {epoch + 1}/{epochs} ==============')\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                if verbose: print(f'lr: {scheduler.get_lr()}')\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0 \n",
    "            for ii, (X_batch, y_batch) in enumerate(dataloaders[phase], start=1):                               \n",
    "                # Move input and label tensors to the GPU\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                # Reset the gradients because they are accumulated\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(X_batch).squeeze(1)  # forward prop\n",
    "                    loss = criterion(outputs, y_batch)  # compute loss\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # backward prop\n",
    "                        optimizer.step()  # update the parameters\n",
    "                        \n",
    "                running_loss += loss.item() * X_batch.shape[0]\n",
    "                \n",
    "            ep_loss = running_loss/len(dataloaders[phase].dataset)  # average loss over an epoch\n",
    "            losses[phase].append(ep_loss)\n",
    "            if verbose: print(f' ({phase}) Loss: {ep_loss:.5f}')\n",
    "                        \n",
    "            # Best model by lowest validation loss\n",
    "            if phase == 'valid' and ep_loss < best_loss:\n",
    "                best_loss = ep_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())          \n",
    "        if verbose: print(f'\\nElapsed time: {round(time.time() - t0, 3)} sec\\n')\n",
    "        \n",
    "    print(f'\\nTraining completed in {round(time.time() - t0_tot, 3)} sec')\n",
    "    \n",
    "    # Load the best model weights to the trained model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    model.losses = losses   \n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    y = [train_losses, val_losses]\n",
    "    c = ['C7', 'C9']\n",
    "    labels = ['Train loss', 'Validation loss']\n",
    "    # Plot train_losses and val_losses wrt epochs\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    x = list(range(1, len(train_losses)+1))\n",
    "    for i in range(2):\n",
    "        ax.plot(x, y[i], lw=3, label=labels[i], color=c[i])\n",
    "        ax.set_xlabel('Epoch', fontsize=16)\n",
    "        ax.set_ylabel('Loss', fontsize=16)\n",
    "        ax.set_xticks(range(0, x[-1]+1, 2))  \n",
    "        ax.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders\n",
    "\n",
    "PyTorch provides [tools](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for loading data to a model in parallel using multiprocessing workers. It also allows batching and shuffling the data. So let's create dataloaders for our training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train/validation split\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Convert to pytorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train_part.values).float()\n",
    "X_valid_tensor = torch.from_numpy(X_valid.values).float()\n",
    "y_train_tensor = torch.from_numpy(y_train_part.values).float()\n",
    "y_valid_tensor = torch.from_numpy(y_valid.values).float()\n",
    "X_test_tensor = torch.from_numpy(X_test.values).float()\n",
    "\n",
    "# Create the train and validation dataloaders\n",
    "train_dataset = data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "\n",
    "dataloaders = {'train': data.DataLoader(train_dataset, batch_size=1000, shuffle=True, num_workers=2), \n",
    "               'valid': data.DataLoader(valid_dataset, batch_size=1000, shuffle=False, num_workers=2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and making predictions\n",
    "Let's try a MLP with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(n_input=X_train.shape[1], n_hidden=[200, 100], n_output=1, drop_p=0.4)\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary cross entropy\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.01, weight_decay=0.005)  # alternatevily torch.optim.SGD(mlp.parameters(), lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "epochs = 12\n",
    "train(mlp, epochs, criterion, optimizer, scheduler, dataloaders, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the train and valid losses wrt epochs to see if both are decreasing. Note that when the training loss is decreasing while the valid loss is increasing it's a sign of overfiting, so perhaps try to tune regularization hyperparameters such as dropout probability and the optimizer's weight_decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(mlp.losses['train'], mlp.losses['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_valid.values, mlp.predict_proba(X_valid_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pretty good result obtained after just ~10 seconds of training... Pytorch provides an easy way of [saving and loading](https://pytorch.org/tutorials/beginner/saving_loading_models.html) our trained model (it's state-dictionary with all the learned weights and biases), so that we don't have to train  it from the beginning each time we want to use it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(mlp.state_dict(), 'mlp.pth')\n",
    "\n",
    "# Load\n",
    "mlp =  MLP(n_input=X_train.shape[1], n_hidden=[200, 100], n_output=1, drop_p=0.4)\n",
    "mlp.load_state_dict(torch.load('mlp.pth'))\n",
    "mlp.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finall, let's make predictions on the test dataset and write to submission file.[](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pred = mlp.predict_proba(X_test_tensor)\n",
    "\n",
    "write_to_submission_file(mlp_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do\n",
    "- Feature engineering. Create new features from the given ones (e.g. `radiant_total_gold - dire_total_gold` etc.), exctract features from the provided json files, perhaps remove some features.\n",
    "- Hyperparameter tunning. Try to find optimal hyperparameters such as number of hidden layers and nodes, learning rate, number of epochs, optimizer and scheduler parameters etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
